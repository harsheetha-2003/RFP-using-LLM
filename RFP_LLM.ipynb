{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_pOK9P7-v_4",
        "outputId": "f898c527-7313-4861-d955-6b8411a45627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB1JpgUM-2-8",
        "outputId": "5525424a-e6c4-44f8-ac30-d8200f702826"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.15.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.15.0-py3-none-any.whl (109 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def process_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text()\n",
        "        print(\"PDF Text Extracted Successfully.\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_html(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from an HTML file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            html_content = file.read()\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        text = soup.get_text(separator=\"\\n\")\n",
        "        title = soup.find('title').text if soup.find('title') else \"No Title Found\"\n",
        "        print(f\"HTML Title Extracted: {title}\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing HTML: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_info_with_groq(document_text, model=\"llama-3.3-70b-versatile\"):\n",
        "    \"\"\"\n",
        "    Send document text to Groq API and extract structured information.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Extract the following information from this document:\n",
        "    - Bid Number\n",
        "    - Title\n",
        "    - Due Date\n",
        "    - Bid Submission Type\n",
        "    - Term of Bid\n",
        "    - Pre-Bid Meeting\n",
        "    - Installation\n",
        "    - Bid Bond Requirement\n",
        "    - Delivery Date\n",
        "    - Payment Terms\n",
        "    - Any Additional Documentation Required\n",
        "    - MFG for Registration\n",
        "    - Contract or Cooperative to use\n",
        "    - Model_no\n",
        "    - Part_no\n",
        "    - Product\n",
        "    - contact_info\n",
        "    - company_name\n",
        "    - Bid Summary\n",
        "    - Product Specification\n",
        "\n",
        "    Document:\n",
        "    {document_text}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant skilled in extracting structured data.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            model=model,\n",
        "            temperature=0.5,\n",
        "            max_completion_tokens=1024,\n",
        "            top_p=1,\n",
        "            stop=None,\n",
        "            stream=False,\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Groq API: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_to_table(extracted_info, output_file):\n",
        "    \"\"\"\n",
        "    Save extracted information into a CSV or table format.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = {\"Field\": [], \"Value\": []}\n",
        "        for line in extracted_info.split(\"\\n\"):\n",
        "            if \":\" in line:\n",
        "                field, value = line.split(\":\", 1)\n",
        "                data[\"Field\"].append(field.strip())\n",
        "                data[\"Value\"].append(value.strip())\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_csv(output_file, index=False)\n",
        "        print(f\"Data saved to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving data to table: {e}\")\n",
        "\n",
        "def process_document(file_path, file_type, output_file):\n",
        "    \"\"\"\n",
        "    Process a single document and save the extracted information to a table.\n",
        "    \"\"\"\n",
        "    if file_type == \"pdf\":\n",
        "        document_text = process_pdf(file_path)\n",
        "    elif file_type == \"html\":\n",
        "        document_text = process_html(file_path)\n",
        "    else:\n",
        "        print(\"Unsupported file type.\")\n",
        "        return\n",
        "\n",
        "    if document_text:\n",
        "        extracted_info = extract_info_with_groq(document_text)\n",
        "        if extracted_info:\n",
        "            save_to_table(extracted_info, output_file)\n",
        "        else:\n",
        "            print(\"No information extracted.\")\n",
        "    else:\n",
        "        print(\"No text extracted from document.\")\n",
        "\n",
        "# Example usage:\n",
        "# For a PDF file\n",
        "process_document(\"/content/Addendum 1 RFP JA-207652 Student and Staff Computing Devices.pdf\", \"pdf\", \"output_pdf.csv\")\n",
        "\n",
        "# For an HTML file\n",
        "process_document(\"//content/Student and Staff Computing Devices __SOURCING #168884__ - Bid Information - {3} _ BidNet Direct.html\", \"html\", \"output_html.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb5V-1Kp-4FT",
        "outputId": "daae6b39-a570-42a9-e540-8ad58d02a673"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF Text Extracted Successfully.\n",
            "Error querying Groq API: name 'client' is not defined\n",
            "No information extracted.\n",
            "HTML Title Extracted: Student and Staff Computing Devices **SOURCING #168884** - Bid Information - {3} | BidNet Direct\n",
            "Error querying Groq API: name 'client' is not defined\n",
            "No information extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-dotenv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jugj-rWR_4Q8",
        "outputId": "cb7e69d6-2c19-4f3d-a4b4-20753e8927a7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "import pdfplumber\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Set up Groq API client\n",
        "api_key = \"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\"  # Replace with your actual API key\n",
        "client = Groq(api_key=api_key)\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to parse HTML and extract information\n",
        "def extract_info_from_html(html_path):\n",
        "    try:\n",
        "        with open(html_path, 'r', encoding='utf-8') as file:\n",
        "            html_content = file.read()\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        title = soup.find('title').text if soup.find('title') else \"No title found\"\n",
        "        return {\"title\": title, \"html_content\": html_content}\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting information from HTML: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to query Groq API\n",
        "def extract_info_with_groq(text):\n",
        "    prompt = f\"\"\"\n",
        "    Extract the following information from this document:\n",
        "    - Bid Number\n",
        "    - Title\n",
        "    - Due Date\n",
        "    - Bid Submission Type\n",
        "    - Term of Bid\n",
        "    - Pre-Bid Meeting\n",
        "    - Installation\n",
        "    - Bid Bond Requirement\n",
        "    - Delivery Date\n",
        "    - Payment Terms\n",
        "    - Any Additional Documentation Required\n",
        "    - MFG for Registration\n",
        "    - Contract or Cooperative to use\n",
        "    - Model_no\n",
        "    - Part_no\n",
        "    - Product\n",
        "    - contact_info\n",
        "    - company_name\n",
        "    - Bid Summary\n",
        "    - Product Specification\n",
        "\n",
        "    Document:\n",
        "    {text}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant skilled in extracting structured data.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            model=\"llama-3.3-70b-versatile\",  # Replace with the desired model\n",
        "            temperature=0.5,\n",
        "            max_completion_tokens=1024,\n",
        "            top_p=1,\n",
        "            stop=None,\n",
        "            stream=False,\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Groq API: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to input files\n",
        "pdf_path = \"/content/Addendum 1 RFP JA-207652 Student and Staff Computing Devices.pdf\"\n",
        "html_path = \"/content/Student and Staff Computing Devices __SOURCING #168884__ - Bid Information - {3} _ BidNet Direct.html\"\n",
        "\n",
        "# Extract text from PDF\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "if pdf_text:\n",
        "    print(\"PDF Text Extracted Successfully.\")\n",
        "\n",
        "# Extract information from HTML\n",
        "html_info = extract_info_from_html(html_path)\n",
        "if html_info:\n",
        "    print(f\"HTML Title Extracted: {html_info['title']}\")\n",
        "\n",
        "# Combine PDF and HTML content for Groq\n",
        "combined_text = pdf_text + \"\\n\\n\" + html_info[\"html_content\"] if pdf_text and html_info else pdf_text or html_info\n",
        "\n",
        "# Query Groq API for structured data\n",
        "if combined_text:\n",
        "    extracted_info = extract_info_with_groq(combined_text)\n",
        "    print(\"Extracted Information:\")\n",
        "    print(extracted_info)\n",
        "else:\n",
        "    print(\"No content to process with Groq.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR0tTi5_BXUU",
        "outputId": "50604499-bab8-4d46-d214-1f43ef71c6eb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF Text Extracted Successfully.\n",
            "HTML Title Extracted: Student and Staff Computing Devices **SOURCING #168884** - Bid Information - {3} | BidNet Direct\n",
            "Error querying Groq API: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01jenh9152ea4bk6asc1ndave2` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 24992, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Extracted Information:\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def process_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text()\n",
        "        print(\"PDF Text Extracted Successfully.\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_html(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from an HTML file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            html_content = file.read()\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        text = soup.get_text(separator=\"\\n\")\n",
        "        title = soup.find('title').text if soup.find('title') else \"No Title Found\"\n",
        "        print(f\"HTML Title Extracted: {title}\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing HTML: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_info_with_groq(document_text, model=\"llama-3.3-70b-versatile\"):\n",
        "    \"\"\"\n",
        "    Send document text to Groq API and extract structured information.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Extract the following information from this document:\n",
        "    - Bid Number\n",
        "    - Title\n",
        "    - Due Date\n",
        "    - Bid Submission Type\n",
        "    - Term of Bid\n",
        "    - Pre-Bid Meeting\n",
        "    - Installation\n",
        "    - Bid Bond Requirement\n",
        "    - Delivery Date\n",
        "    - Payment Terms\n",
        "    - Any Additional Documentation Required\n",
        "    - MFG for Registration\n",
        "    - Contract or Cooperative to use\n",
        "    - Model_no\n",
        "    - Part_no\n",
        "    - Product\n",
        "    - contact_info\n",
        "    - company_name\n",
        "    - Bid Summary\n",
        "    - Product Specification\n",
        "\n",
        "    Document:\n",
        "    {document_text}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant skilled in extracting structured data.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            model=model,\n",
        "            temperature=0.5,\n",
        "            max_completion_tokens=1024,\n",
        "            top_p=1,\n",
        "            stop=None,\n",
        "            stream=False,\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Groq API: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_to_table(extracted_info, output_file):\n",
        "    \"\"\"\n",
        "    Save extracted information into a CSV or table format.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = {\"Field\": [], \"Value\": []}\n",
        "        for line in extracted_info.split(\"\\n\"):\n",
        "            if \":\" in line:\n",
        "                field, value = line.split(\":\", 1)\n",
        "                data[\"Field\"].append(field.strip())\n",
        "                data[\"Value\"].append(value.strip())\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_csv(output_file, index=False)\n",
        "        print(f\"Data saved to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving data to table: {e}\")\n",
        "\n",
        "def process_document(file_path, file_type, output_file):\n",
        "    \"\"\"\n",
        "    Process a single document and save the extracted information to a table.\n",
        "    \"\"\"\n",
        "    if file_type == \"pdf\":\n",
        "        document_text = process_pdf(file_path)\n",
        "    elif file_type == \"html\":\n",
        "        document_text = process_html(file_path)\n",
        "    else:\n",
        "        print(\"Unsupported file type.\")\n",
        "        return\n",
        "\n",
        "    if document_text:\n",
        "        extracted_info = extract_info_with_groq(document_text)\n",
        "        if extracted_info:\n",
        "            save_to_table(extracted_info, output_file)\n",
        "        else:\n",
        "            print(\"No information extracted.\")\n",
        "    else:\n",
        "        print(\"No text extracted from document.\")\n",
        "\n",
        "# Example usage:\n",
        "# For a PDF file\n",
        "process_document(\"/content/Addendum 1 RFP JA-207652 Student and Staff Computing Devices.pdf\", \"pdf\", \"output_pdf.csv\")\n",
        "\n",
        "# For an HTML file\n",
        "process_document(\"//content/Student and Staff Computing Devices __SOURCING #168884__ - Bid Information - {3} _ BidNet Direct.html\", \"html\", \"output_html.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMQnrlMT_dSj",
        "outputId": "79a0c9ba-0429-47bf-90de-a171f190a7a6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF Text Extracted Successfully.\n",
            "Data saved to output_pdf.csv\n",
            "HTML Title Extracted: Student and Staff Computing Devices **SOURCING #168884** - Bid Information - {3} | BidNet Direct\n",
            "Data saved to output_html.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests  # Assuming you are using HTTP requests for the Groq API\n",
        "\n",
        "# Replace with your Groq API endpoint and key\n",
        "GROQ_API_URL = \"https://api.groq.com/v1/extract\"\n",
        "API_KEY = \"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\"\n",
        "\n",
        "def process_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text()\n",
        "        print(\"PDF Text Extracted Successfully.\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_html(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from an HTML file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            html_content = file.read()\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        text = soup.get_text(separator=\"\\n\")\n",
        "        title = soup.find('title').text if soup.find('title') else \"No Title Found\"\n",
        "        print(f\"HTML Title Extracted: {title}\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing HTML: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_info_with_groq(document_text):\n",
        "    \"\"\"\n",
        "    Send document text to Groq API and extract structured information.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"document_text\": document_text\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(GROQ_API_URL, json=data, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()  # Assuming the API returns structured JSON data\n",
        "        else:\n",
        "            print(f\"Error querying Groq API: {response.text}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Groq API: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_to_json(extracted_info, output_file):\n",
        "    \"\"\"\n",
        "    Save extracted information into a JSON format.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as json_file:\n",
        "            json.dump(extracted_info, json_file, ensure_ascii=False, indent=4)\n",
        "        print(f\"Data saved to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving data to JSON: {e}\")\n",
        "\n",
        "def process_document(file_path, file_type, output_file):\n",
        "    \"\"\"\n",
        "    Process a single document and save the extracted information to a JSON file.\n",
        "    \"\"\"\n",
        "    if file_type == \"pdf\":\n",
        "        document_text = process_pdf(file_path)\n",
        "    elif file_type == \"html\":\n",
        "        document_text = process_html(file_path)\n",
        "    else:\n",
        "        print(\"Unsupported file type.\")\n",
        "        return\n",
        "\n",
        "    if document_text:\n",
        "        extracted_info = extract_info_with_groq(document_text)\n",
        "        if extracted_info:\n",
        "            save_to_json(extracted_info, output_file)\n",
        "        else:\n",
        "            print(\"No information extracted.\")\n",
        "    else:\n",
        "        print(\"No text extracted from document.\")\n",
        "\n",
        "# Example usage:\n",
        "# For a PDF file\n",
        "process_document(\"/content/Addendum 1 RFP JA-207652 Student and Staff Computing Devices.pdf\", \"pdf\", \"output_pdf.json\")\n",
        "\n",
        "# For an HTML file\n",
        "process_document(\"//content/Student and Staff Computing Devices __SOURCING #168884__ - Bid Information - {3} _ BidNet Direct.html\", \"html\", \"output_html.json\")\n"
      ],
      "metadata": {
        "id": "JzQGLG4K_eIK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}